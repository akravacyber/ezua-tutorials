{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# DreamBooth\n",
    "\n",
    "This Notebook walks you through the process of implementing DreamBooth, using Stable diffusion. [DreamBooth](https://dreambooth.github.io/) is a pioneering project from Google, designed with the intent of personalizing the outcomes generated by large text-to-image models. As an example, you can refine the generative model utilizing a handful of your dog's pictures and subsequently command it to generate innovative images wherein your dog is the central character.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"images/dreambooth.png\" alt=\"dreambooth\" style=\"width:100%\">\n",
    "    <figcaption>\n",
    "      Image take from <a href=\"(https://dreambooth.github.io/\">DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation</a>\n",
    "    </figcaption>\n",
    "</figure>\n",
    "\n",
    "The original DreamBooth project uses the [Imagen](https://imagen.research.google/) text-to-image model. However, given that Imagen is not an open-source model, the wider community has pivoted towards using Stable Diffusion to develop applications akin to DreamBooth. To this end, this tutorial is designed to guide you in creating your own version of DreamBooth, utilizing Stable Diffusion.\n",
    "\n",
    "Let's import the libraries we need to start the process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "\n",
    "from base64 import b64encode\n",
    "from functools import partial\n",
    "from argparse import Namespace\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import bitsandbytes as bnb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from IPython.display import HTML\n",
    "from huggingface_hub import notebook_login\n",
    "from diffusers import LMSDiscreteScheduler\n",
    "from diffusers import StableDiffusionPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The subsequent code cell establishes the parameters for the training process. For this instance, you will be working with images of size 512 x 512 x 3. This dimension necessitates considerable GPU memory, and to moderate the memory usage, you have to:\n",
    "\n",
    "1. Maintain the training batch size at 1, thereby loading only a single image in the memory at any given moment.\n",
    "1. Configure gradient accumulation to 2, updating the model weights after every two training steps, effectively emulating a training process with a batch size of 2.\n",
    "1. Utilize mixed-precision training (for more info about this technique, see [here](https://pytorch.org/blog/what-every-user-should-know-about-mixed-precision-training-in-pytorch/)).\n",
    "\n",
    "In addition, you delineate the text prompt you will be using to describe the novel concept you aim to teach to your model. Typically, you should search for an infrequently used token and link your concept to it. We will be talking about tokens more down the line. The token `sks` is used quite often for this purpose. Choosing this uncommon token will not impair the model's generalization capacity. Hence, you can employ the prompt `A photo of sks <class>`. The `<class>` token is contingent upon the nature of your concept. For instance, if you are teaching the model to understand a human face, the class can be represented by the term `person`. Consequently, the prompt transforms to `A photo of sks person`. Our model will discard any previous meaning associated with the token `sks`, but this is not a concern for our purposes.\n",
    "\n",
    "Finally, you also set the repository where the pre-trained Stable Diffusion model lives. The model is hosted on Hugging Face Hub. To be able to download it, you should:\n",
    "\n",
    "1. Create an account with [Hugging Face](https://huggingface.co/).\n",
    "1. Go to the [model card](https://huggingface.co/CompVis/stable-diffusion-v1-4) and accept the terms.\n",
    "1. Go to your settings and create an access token with `read` permissions (if you want to upload and store your fine-tuned model on Hugging Face Hub, create an access token with `write` permissions instead).\n",
    "1. Uncomment and run the following cell to log in with your access token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os; os.environ[\"HTTPS_PROXY\"] = \"http://hpeproxy.its.hpecorp.net:80\"\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "IMG_SIZE = 512\n",
    "LEARNING_RATE = 6e-06\n",
    "MAX_TRAINING_STEPS = 450\n",
    "TRAIN_BATCH_SIZE = 1\n",
    "GRADIENT_ACCUMULATION = 2\n",
    "MAX_GRAD_NORM = 1.0\n",
    "INFERENCE_STEPS = 25\n",
    "GUIDANCE_SCALE = 7.5\n",
    "REVISION = \"fp16\"\n",
    "OUTPUT_DIR = \"dreambooth-concept\"\n",
    "MODEL_NAME = \"stabilityai/stable-diffusion-2-1\"\n",
    "CONCEPT_PROMPT = \"A portrait of sks person\"\n",
    "CONCEPT_DATA_PATH = \"/home/dimpo/ezua-tutorials/E2E-Demos/DreamBooth/concept\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    model_name=MODEL_NAME,\n",
    "    img_size=IMG_SIZE,\n",
    "    concept_data_path=CONCEPT_DATA_PATH,\n",
    "    concept_prompt=CONCEPT_PROMPT,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    max_train_steps=MAX_TRAINING_STEPS,\n",
    "    train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    gradient_accumulation = GRADIENT_ACCUMULATION,\n",
    "    max_grad_norm=MAX_GRAD_NORM,\n",
    "    revision=REVISION,\n",
    "    gradient_checkpointing=True,\n",
    "    eight_bit_optimizer=True,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    seed=42,\n",
    "    save_concept=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## The CLIP Tokenizer\n",
    "\n",
    "We are now ready to delve into the functionality of the Stable Diffusion model. Comprised of three components, Stable Diffusion includes:\n",
    "\n",
    "1. A U-Net model\n",
    "1. A CLIP encoder model\n",
    "1. A Variational Autoencoder model\n",
    "\n",
    "Let's take things one step at a time and explore the [CLIP](https://openai.com/blog/clip/) encoder first. Stable Diffusion employs a CLIP encoder to convert the words in our prompt into semantically meaningful vectors. We'll divide this encoding process into two stages: i) the tokenization stage and ii) the encoding stage.\n",
    "\n",
    "The preliminary part of the process is termed tokenization. The CLIP tokenizer functions as a dictionary that associates a word with a specific integer ID. Let's examine how this operates with our given prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import CLIPTokenizer\n",
    "\n",
    "def get_clip_tokenizer(model_name: str, revision: str) -> CLIPTokenizer:\n",
    "    \"\"\"Get the CLIP tokenizer.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): The name of the pretrained model to load.\n",
    "        revision (str): The revision of the model to load.\n",
    "\n",
    "    Returns:\n",
    "        CLIPTokenizer: The CLIP tokenizer.\n",
    "    \"\"\"\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(\n",
    "        model_name, subfolder=\"tokenizer\", revision=revision)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load the CLIP tokenizer\n",
    "clip_tokenizer = get_clip_tokenizer(args.model_name, args.revision)\n",
    "# return the tokens as PyTorch tensors\n",
    "tokens = clip_tokenizer(CONCEPT_PROMPT, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Concept prompt: '{CONCEPT_PROMPT}'\")\n",
    "print(f\"Tokens: {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "You see that the tokenizer turned the prompt \"A photo of sks person\" into a sequence of integers:\n",
    "\n",
    "`49406, 320, 5352, 539, 48136, 2533, 49407`\n",
    "\n",
    "You observe that we have `7` tokens for `5` words. Why is that? Furthermore, what is this `attention_mask` key you see there? First, let's convert each ID back to words, so you can examine to what word each of the tokens correspond:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for token in tokens[\"input_ids\"][0]:\n",
    "    print(f\"Token ID: {token}\\t Word: {clip_tokenizer.decoder.get(int(token))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "We see that the CLIP tokenizer reserves IDs `49406` and `49407` to indicate the start and end of text sequences. We also see that each word has a special end-of-word suffix (`</w>`). This special suffix is used in some tokenization methods as a way to indicate the end of a word.\n",
    "\n",
    "Now, what is this `attention_mask` list? This mask is a list of integers that the CLIP encoder uses down the road. It informs the model about the words in the sequence it needs to attend to. So, if we had used padding to change the length of this sequence, the `attention_mask` would have zeros in the padding positions to inform the model that it does not have to consider the padding tokens when making predictions. Let's do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "padded_tokens = clip_tokenizer(CONCEPT_PROMPT, padding=\"max_length\",\n",
    "                               max_length=clip_tokenizer.model_max_length,\n",
    "                               return_tensors=\"pt\")\n",
    "\n",
    "print(f\"Max sequence lenght the tokenizer can handle: {clip_tokenizer.model_max_length}\")\n",
    "print(f\"Concept prompt: '{CONCEPT_PROMPT}'\")\n",
    "print(f\"Tokens: {padded_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The tokenizer is capable of managing sequences that extend up to a maximum of 77 tokens. In the event that padding is required, it adds a long sequence of `<|endoftext|>` tokens. Conversely, if the sentence surpasses 77 words in length, it truncates the sentence. Finally, the attention mask inserts a continuous sequence of zeros at the positions designated for padding.\n",
    "\n",
    "Now, we have a way to convert our words into numbers. Let's move to the encoding part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## The CLIP encoder\n",
    "\n",
    "Stable Diffusion utilizes CLIP to map our text prompts into a multidimensional space. Simply put, it transforms each word in our prompt into vectors, infusing them with meaning for the model. These vectors are referred to as \"embeddings\" and we separately train a CLIP model to generate them. For further insight on this procedure, you may read through this [blog](https://openai.com/blog/clip/) by OpenAI.\n",
    "\n",
    "For our application, it is unnecessary to re-train or fine-tune this component during DreamBooth's training process, as it is already proficient in translating each word in our text prompt into substantial vectors. Let's observe this operation in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import CLIPTextModel\n",
    "\n",
    "def get_clip_encoder(model_name: str, revision: str) -> CLIPTextModel:\n",
    "    \"\"\"Get the CLIP encoder model.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): The name of the pretrained model to load.\n",
    "        revision (str): The revision of the model to load.\n",
    "\n",
    "    Returns:\n",
    "        CLIPTextModel: The CLIP encoder model.\n",
    "    \"\"\"\n",
    "    text_encoder = CLIPTextModel.from_pretrained(\n",
    "        model_name, subfolder=\"text_encoder\", revision=revision)\n",
    "    return text_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# load the CLIP encoder model\n",
    "clip_encoder = get_clip_encoder(args.model_name, args.revision).to(device)\n",
    "# embed the text prompt token ids\n",
    "with torch.no_grad():\n",
    "    text_embeddings = clip_encoder(padded_tokens.input_ids.to(device))[0]\n",
    "\n",
    "print(f\"Embedding shape: {text_embeddings.shape}\")\n",
    "print(f\"Embedding of the word 'photo' (first five elements): {text_embeddings[0][2][:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "As observed, we processed our text prompt through the CLIP tokenizer, which transformed it into a sequence of `7` digits. Subsequently, these digits were routed through the CLIP encoder, morphing them into seven vectors, each of `1024` dimensions. These vectors serve as the input for our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# The Variational Autoencoder\n",
    "\n",
    "Stable Diffusion is a Latent Diffusion Model (LDM). To understand how it works, let's first understand how Diffusion Models learn to generate images. The following steps describe the training process of a diffusion model. Given an image from a training set, a Diffusion Model:\n",
    "\n",
    "1. Adds noise to the image iteratively, until the image information is lost (this is called the forward diffusion process)\n",
    "1. Learns the reverse process: take the noisy image and try to reconstruct the original one\n",
    "\n",
    "<figure>\n",
    "  <img src=\"images/diffusion.png\" alt=\"diffusion\" style=\"width:100%\">\n",
    "    <figcaption>\n",
    "      image taken from: Ho, J., Jain, A. and Abbeel, P., 2020. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33, pp.6840-6851.\n",
    "    </figcaption>\n",
    "</figure>\n",
    "\n",
    "Once the model grasps the inverse procedure (the denoising process), we can introduce random noise as input, enabling it to generate realistic-looking images. Subsequently, we can direct the thematic concept of the generated image using a textual prompt.\n",
    "\n",
    "The complication arises from the fact that the model operates within the pixel space, and processing `512 x 512 x 3` images can render the process rather sluggish. How can we improve this? We can compress the image into a latent (or hidden) representation, retain the properties that define the image, and carry out the training process within this latent space. This space is termed latent (or hidden) because the meaning of its dimensions remains unknown to us. We treat it as a black box; we inform the model of a space encompassing (for example) `16000` dimensions and grant it the liberty to manipulate it as desired.\n",
    "\n",
    "The Variational Autoencoder (VAE) does exactly this: compresses the image into a latent space, but keeps the characterists that makes the image what it is. That's why it can later decompress the latent back into the original image with great accuracy. You can think of a VAE as another lossy compression algorithm.\n",
    "\n",
    "Stable Diffusion uses a VAE to compress the original image into a latent space. That's why Stable Diffusion falls into the category of LDMs. Usually, the encoder component of the autoencoder compresses a `512 x 512 x 3` image to `64 x 64 x 4`, which is a `48x` reduction. Then, the decoder knows how to take a `64 x 64 x 4` latent representation and reconstruct the original image. Working on the latent representation of the image during the fowrward diffusion and denoising steps makes things an order of magnitude faster.\n",
    "\n",
    "We do not need to re-train or fine-tune this component during the training process of DreamBooth, as it already knows how to compress an image to a latent space effectively. However, let's see the autoencoder in action. First, let's load the image that contains the concept we want to teach our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image = Image.open(os.path.join(\"images\", \"dog.png\"))\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's convert the image to a PyTorch tensor and pass it through the autoencoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from diffusers import AutoencoderKL\n",
    "\n",
    "def get_vae(model_name: str, revision: str) -> AutoencoderKL:\n",
    "    \"\"\"Get the VAE model.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): The name of the pretrained model to load.\n",
    "        revision (str): The revision of the model to load.\n",
    "\n",
    "    Returns:\n",
    "        AutoencoderKL: The VAE model.\n",
    "    \"\"\"\n",
    "    vae = AutoencoderKL.from_pretrained(\n",
    "        model_name, subfolder=\"vae\", revision=revision)\n",
    "    return vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "# convert image to a PyTorch tensor\n",
    "transform_func = transforms.Compose([transforms.ToTensor()])\n",
    "tensor_image = transform_func(image)\n",
    "\n",
    "# encode the image with the VAE encoder\n",
    "vae = get_vae(args.model_name, args.revision).to(device)\n",
    "with torch.no_grad():\n",
    "    latent = vae.encode(tensor_image.unsqueeze(0).to(device) * 2 - 1)\n",
    "    latent = 0.18215 * latent.latent_dist.sample()\n",
    "\n",
    "print(f\"Latent shape: {latent.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The autoencoder compressed the `512 x 512 x 3` image to a `64 x 64 x 4` latent representation. The unit dimension at the beginning indicates tha batch size. Here we only have a single image hence `1 x 64 x 64 x 4`. Let's visualize the four channels of the latent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "for c in range(4):  # display each channel separately\n",
    "    ax[c].imshow(latent.cpu()[0][c], cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Since the latents are in a matrix form, we can visualize them as images. It seems like the encoder downsampled the initial image quite a bit, however, even if they do look like images we cannot be sure what are the characteristics of the original image that each pixel or channel encodes. The original image is not embedded into a latent space.\n",
    "\n",
    "Finally, let's decompress the image and get back the uncompressed one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "latent = (1 / 0.18215) * latent\n",
    "\n",
    "# decompress the image with the VAE decoder\n",
    "with torch.no_grad():\n",
    "    tensor_image = vae.decode(latent).sample\n",
    "\n",
    "tensor_image = (tensor_image / 2 + 0.5).clamp(0, 1)\n",
    "tensor_image = tensor_image.detach().permute(0, 2, 3, 1).squeeze().cpu().numpy()\n",
    "\n",
    "tensor_image = (tensor_image * 255).round().astype(\"uint8\")\n",
    "reconstruction = Image.fromarray(tensor_image)\n",
    "reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Even if you squint you can't really tell how the reconstructed image differs from the original one! So let's compare the original image with the reconstruction side by side:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def image_grid(imgs, rows, cols):\n",
    "    \"\"\"Create a grid of images\"\"\"\n",
    "    width, height = imgs[0].size\n",
    "    grid = Image.new('RGB', size=(cols * width, rows * height))\n",
    "\n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i % cols * width, i // cols * height))\n",
    "\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import ImageChops\n",
    "\n",
    "diff = ImageChops.difference(image, reconstruction)\n",
    "image_grid([image, reconstruction, diff], rows=1, cols=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "As observed, the difference is negligible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## The U-Net\n",
    "\n",
    "The final piece of Stable Diffusion is the U-Net model. The goal of the U-Net is to learn the reverse function of the forward diffusion process. It does this by predicting the amount of noise we added to the latent representation of the image at each step of the forward diffusion process. Then, we can subtract the predicted noise from the noisy latent to get the original one.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"images/u-net.png\" alt=\"u-net\" style=\"width:50%\">\n",
    "    <figcaption>\n",
    "      The U-Net architecture first proposed in Ronneberger, O., Fischer, P. and Brox, T., 2015, October. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention (pp. 234-241). Springer, Cham.\n",
    "    </figcaption>\n",
    "</figure>\n",
    "\n",
    "However, instead of performing the denoising operation in one step, we do this iteratively, removing a small amount of noise at each step. This tends to produce more accurate results, as the model does not have a lot to work with when starting from random noise. Later in the Notebook we'll see how that process works, but first, we need a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create the dataset\n",
    "\n",
    "Our next step is to create the examples we'll use during training. An example has two parts: i) the image and ii) the text prompt. Then, we need a function to stack our examples on top of each other and create batches. That is what the `_collate_fn` is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class DreamBoothDataset(Dataset):\n",
    "    \"\"\"The DreamBooth dataset.\n",
    "    \n",
    "    This dataset loads the concept images from the concept data path and\n",
    "    creates training examples by combining the concept images with the\n",
    "    concept prompt.\n",
    "\n",
    "    Args:\n",
    "        concept_data_path (str): The path to the concept images.\n",
    "        concept_prompt (str): The concept prompt to use.\n",
    "        tokenizer (str): The CLIP tokenizer.\n",
    "        img_size (int, optional): The image size to use. Defaults to 512.\n",
    "    \n",
    "    Attributes:\n",
    "        transforms (torchvision.transforms.Compose): The image transformation\n",
    "            composition.\n",
    "    \"\"\"\n",
    "    def __init__(self, concept_data_path: str, concept_prompt: str,\n",
    "                 tokenizer: str, img_size: int = 512):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.concept_prompt = concept_prompt\n",
    "        self.concept_images = [fn for fn in Path(concept_data_path).iterdir()\n",
    "                               if str(fn).lower().endswith(('.jpg', '.jpeg'))]\n",
    "\n",
    "        self.image_transforms = transforms.Compose([\n",
    "                transforms.Resize(img_size, \n",
    "                    interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "                transforms.CenterCrop(img_size),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.5], [0.5])])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.concept_images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        example = {}\n",
    "\n",
    "        # create the concept images part of the example\n",
    "        concept_image = Image.open(\n",
    "            self.concept_images[index % len(self.concept_images)])\n",
    "\n",
    "        if not concept_image.mode == \"RGB\":\n",
    "            concept_image = concept_image.convert(\"RGB\")\n",
    "\n",
    "        example[\"concept_images\"] = self.image_transforms(concept_image)\n",
    "\n",
    "        # create the concept prompt part of the example\n",
    "        concept_prompt_tokens = self.tokenizer(\n",
    "            self.concept_prompt, padding=\"do_not_pad\", truncation=True,\n",
    "            max_length=self.tokenizer.model_max_length).input_ids\n",
    "\n",
    "        example[\"concept_prompt_tokens\"] = concept_prompt_tokens\n",
    "        \n",
    "        return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _collate_fn(examples, tokenizer):\n",
    "    # for each example, extract the text prompts and the images\n",
    "    input_ids = [example[\"concept_prompt_tokens\"] for example in examples]\n",
    "    pixel_values = [example[\"concept_images\"] for example in examples]\n",
    "\n",
    "    # stack the pixel values vertically\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    pixel_values = pixel_values.contiguous().float()\n",
    "\n",
    "    # pad the text prompt tokens\n",
    "    input_ids = tokenizer.pad(\n",
    "        {\"input_ids\": input_ids}, padding=True, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    # return a batch of padded tokens and images\n",
    "    batch = {\"input_ids\": input_ids,\n",
    "             \"pixel_values\": pixel_values}\n",
    "\n",
    "    return batch\n",
    "\n",
    "collate_fn = partial(_collate_fn, tokenizer=clip_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The `Dataset` defines what each example consists of. In our case, one example consists of an image and a tokenized prompt. Finally, the `DataLoader` retrieves a few examples from the dataset and passes them to the collate function to create batches. It does this in a lazy manner and in our case, we've set the batch size to `1`, so the `DataLoader` tells the collate function to stack just one image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = DreamBoothDataset(\n",
    "        concept_data_path=args.concept_data_path,\n",
    "        concept_prompt=args.concept_prompt,\n",
    "        tokenizer=clip_tokenizer,\n",
    "        img_size=args.img_size)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=args.train_batch_size,\n",
    "        shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's get one of our examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "example = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's first examine the size of the image that the example contains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from the example tuple get the image\n",
    "img = example[\"pixel_values\"]\n",
    "# the image is normalized to have values between -1 and 1\n",
    "# clip it, so the min value is `0` and the max `1`\n",
    "img = torch.clip(img, min=0., max=1.,)\n",
    "# remove the batch dimension and move the channel dimension last\n",
    "img = img.squeeze().permute(1, 2, 0).numpy()\n",
    "\n",
    "print(f\"Image size: {img.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Our data is what we expected them to be: a `512 x 512 x 3` image. Now, let's see what our image looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# rescale image to [0...255]\n",
    "img = (img * 255).round().astype(\"uint8\")\n",
    "# load and display the image\n",
    "Image.fromarray(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The image now looks a bit different from the original image we saw earlier. This is because the Dataset introduces a series of transformations to the original image to make it easier to work with:\n",
    "\n",
    "- Center crops it so it is a square image\n",
    "- Normalizes the image so each pixel takes values in range [-1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## The Scheduler\n",
    "\n",
    "Let's take a look now at the forward diffusion process we mentioned earlier. During this process, we iteratively add noise to the image until it becomes just random noise. But how we do that? What iteratively means?\n",
    "\n",
    "Let's see how the graph of the scheduler looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scheduler = LMSDiscreteScheduler(\n",
    "    beta_start=0.00085, beta_end=0.012,\n",
    "    beta_schedule=\"scaled_linear\")\n",
    "\n",
    "plt.plot(scheduler.sigmas)\n",
    "plt.title(\"Noise Schedule\")\n",
    "plt.xlabel(\"Sampling step\")\n",
    "plt.ylabel(\"$\\sigma$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The scheduler is a function that accepts a sampling step and returns the standard deviation that we use to scale the noise. In plain english, it decides the amount of noise we add to the image at each sampling step. From the graph, we can see that at step `0` we add a lot of noise to the image, and at step `1000` we add just a little.\n",
    "\n",
    "In general, the training process follows the steps below:\n",
    "\n",
    "1. Take an image from the dataset\n",
    "1. Compress the image to a latent representation\n",
    "1. Sample a random integer to indicate the timestep\n",
    "1. Pass the timestep to the scheduler and get the amount of noise to add to the latent\n",
    "1. Add the noise to the latent (forward diffusion process)\n",
    "1. Pass the noisy latent through the U-Net and get a pred\n",
    "\n",
    "Let's see how our original image would look like if we added the amount of noise that the scheduler returns for timestep `600`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# random noise\n",
    "noise = torch.randn_like(latent) # Random noise\n",
    "# set timestep to `600`\n",
    "timestep = 600\n",
    "# add noise to the image\n",
    "# the scheduler scaled the random noise by sigma and adds it to the image\n",
    "# noisy_image = original_image + noise * sigma\n",
    "noisy_latent = scheduler.add_noise(latent, noise, timesteps=torch.tensor([scheduler.timesteps[timestep]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's visualize the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import ImageFile\n",
    "\n",
    "def latent_to_image(latent: torch.tensor,\n",
    "                    vae: torch.nn.Module = None) -> ImageFile:\n",
    "    \"\"\"Visualize a latent as a PIL image.\n",
    "    \n",
    "    Args:\n",
    "        latent (torch.tensor): The latent representation.\n",
    "        decoder (torch.nn.Module): A decoder module to\n",
    "            decode the latent back to pixel space.\n",
    "    \n",
    "    Return:\n",
    "        ImageFile: A PIL ImageFile object.\n",
    "    \"\"\"\n",
    "    if vae:\n",
    "        latent = (1 / 0.18215) * latent\n",
    "\n",
    "        # decompress the image with the VAE decoder\n",
    "        with torch.no_grad():\n",
    "            tensor_image = vae.decode(latent).sample\n",
    "    else:\n",
    "        tensor_image = latent\n",
    "\n",
    "    tensor_image = (tensor_image / 2 + 0.5).clamp(0, 1)\n",
    "    tensor_image = (tensor_image.detach()\n",
    "                                .permute(0, 2, 3, 1)\n",
    "                                .squeeze()\n",
    "                                .cpu()\n",
    "                                .numpy())\n",
    "    tensor_image = (tensor_image * 255).round().astype(\"uint8\")\n",
    "    return Image.fromarray(tensor_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "noisy_image = latent_to_image(noisy_latent, vae)\n",
    "noisy_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training & Inference\n",
    "\n",
    "Let's load the U-Net and set it to training mode. As we said before, this is the only model we need to fine tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from diffusers import UNet2DConditionModel\n",
    "\n",
    "\n",
    "def get_unet(model_name: str, revision: str,\n",
    "             conditioned: bool = True) -> UNet2DConditionModel:\n",
    "    \"\"\"Get the UNet model.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): The name of the pretrained model to load.\n",
    "        revision (str): The revision of the model to load.\n",
    "        conditioned (bool, optional): Whether to load the conditioned UNet\n",
    "            model. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        UNet2DConditionModel: The UNet model.\n",
    "    \"\"\"\n",
    "    return UNet2DConditionModel.from_pretrained(\n",
    "        model_name, subfolder=\"unet\", revision=revision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unet = get_unet(args.model_name, args.revision).to(device)\n",
    "unet = unet.train()\n",
    "unet.enable_gradient_checkpointing()  # enable gradient checkpointing to reduce the GPU momory consumption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "For the optimizer, we'll use an `8bit` implementation of [AdamW](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html). We use this specific implementation to reduce the memory footprint of the optimizer on the GPU. You can learn more about `8bit` optimizers by watching this [presentation](https://youtu.be/IxrlHAJtqKE) by the creator of the `bitsandbytes` library, Tim Dettmers. \n",
    "\n",
    "Finally, we pass only the parameters of the U-Net model, so the optimizer can only mess with these parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = bnb.optim.AdamW8bit(unet.parameters(), lr=args.learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's get an example and walk through the inference process for one step. This is just for demonstration. Later, we'll kick-off a real training process on a GPU device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "imgs, input_ids = example[\"pixel_values\"], example[\"input_ids\"]\n",
    "imgs, input_ids = imgs.to(device), input_ids.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Compress the image to a latent representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    latent = vae.encode(imgs).latent_dist.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Add noise to the latent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "noise = torch.randn(latent.shape).to(device)\n",
    "\n",
    "scheduler.set_timesteps(INFERENCE_STEPS)\n",
    "noisy_latent = scheduler.add_noise(latent, noise, torch.tensor([scheduler.timesteps[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Visualize the four dimensions of the noisy latent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1, 4, figsize=(16, 4))\n",
    "for c in range(4):  # display each channel separately\n",
    "    ax[c].imshow(noisy_latent.cpu()[0][c], cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "At this point we can use the scheduler and the prediction of the U-Net to compute the previous latent sample. Let's put all of this in a loop, to see what our model knows about the `sks person` we use in our text prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.makedirs(\"steps\", exist_ok=True)\n",
    "\n",
    "uncond_input = clip_tokenizer(\n",
    "    [\"\"] * args.train_batch_size, padding=\"max_length\",\n",
    "    max_length=clip_tokenizer.model_max_length, return_tensors=\"pt\"\n",
    ")\n",
    "with torch.no_grad():\n",
    "    uncond_embeddings = clip_encoder(uncond_input.input_ids.to(device))[0]\n",
    "\n",
    "text_embeddings_concat = torch.cat([uncond_embeddings, text_embeddings])\n",
    "\n",
    "for i, t in enumerate(tqdm(scheduler.timesteps)):\n",
    "    input_latent = torch.cat([noisy_latent] * 2)\n",
    "    input_latent = scheduler.scale_model_input(input_latent, t)\n",
    "\n",
    "    # predict the noise residual\n",
    "    noise_pred = unet(input_latent, t, encoder_hidden_states=text_embeddings_concat).sample\n",
    "        \n",
    "    # perform guidance\n",
    "    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "    noise_pred = noise_pred_uncond + GUIDANCE_SCALE * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "    # compute the previous noisy sample x_t -> x_t-1\n",
    "    noisy_latent = scheduler.step(noise_pred, t, noisy_latent).prev_sample\n",
    "    \n",
    "    # save the result of step i\n",
    "    generated_image = latent_to_image(noisy_latent, vae)\n",
    "    generated_image.save(f'steps/{i:04}.jpeg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "We can now create a sequence of the generated images to observe the denoising process. You can see that the model starts with a very noisy image, where you can barely recognize the shape of a human face. It tries to reconstruct our original image but it does a bad job, since it's not yet trained to associate the work `sks` with the face we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ffmpeg -v 1 -y -f image2 -framerate 1 -i steps/%04d.jpeg -c:v libx264 -preset slow -qp 18 -pix_fmt yuv420p out.mp4\n",
    "\n",
    "mp4 = open('out.mp4','rb').read()\n",
    "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=600 controls>\n",
    "      <source src=\"%s\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\" % data_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "So, let's use the standard training procedure to update the weights of the U-Net, so it can do a bit of a better job next time. First, we need a loss function. The loss function will inform the model how off it is. Since the model predicts the noise we added to each step, we can compare this prediction with the actual noise we added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = F.mse_loss(noise_pred, noise)\n",
    "print(f\"Loss: {loss:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Next, we run the backward propagation algorithm to assign each model weight its part of the blame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Finally, we can use the optimizer to update the model's weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's now fine-tune Stable Diffusion on our custom dataset. At this point your GPU device may be running out of memory. To run the following cell sucessfully, restart the Notebook, and run the \"imports\" cell, as wel as the two cells that define the `args` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from booth.train import train_dreambooth\n",
    "\n",
    "train_dreambooth(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "When the training process completes (it takes `~10` minutes on a Tesla P100), the booth helper library will save the trained model to a `dreambooth-concept` folder. We can load it and generate our own images. Let's see it in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reproducibility\n",
    "torch.manual_seed(1918918)\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\"dreambooth-concept\", torch_dtype=torch.float16).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"a portrait of sks man in game of thrones\"\n",
    "images = pipe(prompt, guidance_scale=13, num_images_per_prompt=2).images\n",
    "\n",
    "image_grid(images, rows=1, cols=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Finally, if you want to store the model to Hugging Face Hub, run the cell below. It will create a new private repository called `dreambooth-frodo` under your account. You can then reference it as `<username>/dreambooth-frodo`.\n",
    "\n",
    "> Note that you need an access token with `write` permissions for this step to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_model = \"dreambooth-concept\"\n",
    "repo_name = \"dreambooth-frodo\"\n",
    "\n",
    "upload_to_huggingface(path=path_to_model,\n",
    "                      name=repo_name,\n",
    "                      prompt=CONCEPT_PROMPT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-dreambooth]",
   "language": "python",
   "name": "conda-env-.conda-dreambooth-py"
  },
  "kubeflow_notebook": {
   "autosnapshot": true,
   "deploy_config": {},
   "docker_image": "gcr.io/arrikto/jupyter-kale-gpu-tf-py38@sha256:8f53a68c798387b9ddab6559ce84b3c36b72a92e09585c32c9371a2438a87f92",
   "experiment": {
    "id": "",
    "name": ""
   },
   "experiment_name": "",
   "katib_metadata": {
    "algorithm": {
     "algorithmName": "grid"
    },
    "maxFailedTrialCount": 3,
    "maxTrialCount": 12,
    "objective": {
     "objectiveMetricName": "",
     "type": "minimize"
    },
    "parallelTrialCount": 3,
    "parameters": []
   },
   "katib_run": false,
   "pipeline_description": "",
   "pipeline_name": "",
   "snapshot_volumes": true,
   "storage_class_name": "",
   "volume_access_mode": "rwm",
   "volumes": [
    {
     "annotations": [],
     "mount_point": "/home/jovyan",
     "name": "learn-ml-workspace-pp7qc",
     "size": 80,
     "size_type": "Gi",
     "snapshot": false,
     "type": "clone"
    }
   ]
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
