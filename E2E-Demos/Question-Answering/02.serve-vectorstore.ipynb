{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48b72df6-4961-4128-9281-6c9634dd33fa",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Creating an Inference Service using MLFlow and KServe\n",
    "\n",
    "Welcome to the second part of our tutorial on building a question-answering application over a corpus of private documents using Large Language Models (LLMs). In the previous Notebook, you focused on embedding the documents into a high-dimensional latent space and storing these embeddings in a Vector Store using the Chroma database interface provided by LangChain.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"images/inference-service.jpg\" alt=\"isvc\" style=\"width:100%\">\n",
    "  <figcaption>\n",
    "    Photo by <a href=\"https://unsplash.com/@growtika?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Growtika</a> on <a href=\"https://unsplash.com/photos/GSiEeoHcNTQ?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Unsplash</a>\n",
    "  </figcaption>\n",
    "</figure>\n",
    "\n",
    "In this Notebook, you will be taking the next step in this journey. You will use MLFlow to log the Chroma DB files that contain our Vector Store as artifacts of an experiment. After logging the artifacts, you will then create an Inference Service that retrieves these artifacts and uses them to provide context to user queries. For this purpose, you'll be using KServe, a Kubernetes-based platform that provides a serverless framework for serving machine learning models at scale.\n",
    "\n",
    "It's important to note that since KServe does not support serving a Chroma DB out-of-the-box, you will be using is a custom predictor component. This means that you'll need to create a Docker image first, which can then be deployed as our Inference Service. This process allows for a high degree of customization, enabling us to fine-tune our service to our specific needs. You can find the code, as well as the Dockerfile for the custom predictor inside the `vectorstore` directory of this project. However, you can use the one we have pre-built for you: `gcr.io/mapr-252711/ezua-demos/vectorstore:1.1`.\n",
    "\n",
    "Once you're ready, this Notebook will guide you through the necessary steps for creating a scalable Vector Store service. First, let's import the libraries you'll need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22867e3-a69c-488a-819e-cced462be9e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import base64\n",
    "import logging\n",
    "import warnings\n",
    "import subprocess\n",
    "\n",
    "import mlflow\n",
    "import requests\n",
    "import boto3\n",
    "import s3transfer\n",
    "import ipywidgets as widgets\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c72b99-3585-49e9-993a-81764e7773a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get all loggers\n",
    "loggers = logging.Logger.manager.loggerDict.values()\n",
    "\n",
    "# Iterate over all loggers and set their level to ERROR\n",
    "# as we don't want to polute the output of the code cells\n",
    "# with debugging messages.\n",
    "for logger in loggers:\n",
    "    if isinstance(logger, logging.Logger):\n",
    "        logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7656fd38-0660-402b-bd83-72e566cd4e0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "def encode_base64(message: str):\n",
    "    encoded_bytes = base64.b64encode(message.encode('ASCII'))\n",
    "    return encoded_bytes.decode('ASCII')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a8b07a-15d8-44a2-89c4-266ee61d1a1e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Logging the Vector Store as an Artifact\n",
    "\n",
    "To kick-off this process, you'll need to provide a few variables:\n",
    "\n",
    "1. The domain of your EzAF cluster (e.g., `hpe-ezaf`)\n",
    "1. Your username and password so you can connect to MLFlow, create experiments, and log artifacts.\n",
    "1. The custom predictor Docker image you built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858c0432-b322-474b-8c34-4244b8e48181",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add heading\n",
    "heading = widgets.HTML(\"<h2>MLflow Credentials</h2>\")\n",
    "display(heading)\n",
    "\n",
    "ezaf_env_input = widgets.Text(description='EZAF Env:')\n",
    "username_input = widgets.Text(description='Username:')\n",
    "password_input = widgets.Password(description='Password:')\n",
    "predictor_input = widgets.Text(description='VectorStore image:')\n",
    "submit_button = widgets.Button(description='Submit')\n",
    "success_message = widgets.Output()\n",
    "\n",
    "ezaf_env = None\n",
    "mlflow_username = None\n",
    "mlflow_password = None\n",
    "predictor = None\n",
    "\n",
    "def submit_button_clicked(b):\n",
    "    global ezaf_env, mlflow_username, mlflow_password, predictor\n",
    "    ezaf_env = ezaf_env_input.value\n",
    "    mlflow_username = username_input.value\n",
    "    mlflow_password = password_input.value\n",
    "    predictor = predictor_input.value\n",
    "    with success_message:\n",
    "        success_message.clear_output()\n",
    "        print(\"Credentials submitted successfully!\")\n",
    "    submit_button.disabled = True\n",
    "\n",
    "submit_button.on_click(submit_button_clicked)\n",
    "\n",
    "# Set margin on the submit button\n",
    "submit_button.layout.margin = '20px 0 20px 0'\n",
    "\n",
    "# Display inputs and button\n",
    "display(ezaf_env_input, username_input, password_input, predictor_input, submit_button, success_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbd664b-9de0-451b-be97-27f446ac5527",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EZAF_ENV = ezaf_env\n",
    "token_url = f\"https://keycloak.{EZAF_ENV}.com/realms/UA/protocol/openid-connect/token\"\n",
    "\n",
    "data = {\n",
    "    \"username\" : mlflow_username,\n",
    "    \"password\" : mlflow_password,\n",
    "    \"grant_type\" : \"password\",\n",
    "    \"client_id\" : \"ua-grant\",\n",
    "}\n",
    "\n",
    "token_responce = requests.post(token_url, data=data, allow_redirects=True, verify=False)\n",
    "\n",
    "token = token_responce.json()[\"access_token\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4f78ad-fbe0-4d97-bd60-c706df8fba90",
   "metadata": {
    "tags": []
   },
   "source": [
    "You'll also need access to the MinIO object store, to log the Chroma DB files. Use the following dialogue to provide your credentials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2410856-f0ab-45d3-87c6-f4d1e05e8409",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add heading\n",
    "heading = widgets.HTML(\"<h2>Object Store Credentials</h2>\")\n",
    "display(heading)\n",
    "\n",
    "# Access Key and Secret Key inputs\n",
    "access_key_input = widgets.Text(description='Access Key:')\n",
    "secret_key_input = widgets.Password(description='Secret Key:')\n",
    "submit_button = widgets.Button(description='Submit')\n",
    "success_message = widgets.Output()\n",
    "\n",
    "minio_access_key = None\n",
    "minio_secret_key = None\n",
    "\n",
    "def submit_button_clicked(b):\n",
    "    global minio_access_key, minio_secret_key\n",
    "    minio_access_key = access_key_input.value\n",
    "    minio_secret_key = secret_key_input.value\n",
    "    with success_message:\n",
    "        success_message.clear_output()\n",
    "        print(\"Credentials submitted successfully!\")\n",
    "    submit_button.disabled = True\n",
    "\n",
    "submit_button.on_click(submit_button_clicked)\n",
    "\n",
    "# Set margin on the submit button\n",
    "submit_button.layout.margin = '20px 0 20px 0'\n",
    "\n",
    "# Display inputs and button\n",
    "display(access_key_input, secret_key_input, submit_button, success_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41272ee-2fa0-4069-a4f9-210bdcf84058",
   "metadata": {
    "tags": []
   },
   "source": [
    "Using the following helper function you are ready to create a new experiment (or re-use and existing one) and log the Chroma DB files as an artifact of this experiment. In the end, you'll need to retrieve the URI pointing to the location of this artifact and pass it to the custom predictor component. This way, the custom predictor component will know hot to retrieve the artifact and serve it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78cc819-06c5-4805-81e5-9bf13f082861",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_or_create_experiment(exp_name):\n",
    "    \"\"\"Register an experiment in MLFlow.\n",
    "    \n",
    "    args:\n",
    "      exp_name (str): The name of the experiment.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        user = \"dimpo\"\n",
    "        os.environ['MLFLOW_TRACKING_INSECURE_TLS'] = \"true\"\n",
    "        os.environ[\"AWS_ACCESS_KEY_ID\"] = minio_access_key\n",
    "        os.environ[\"AWS_SECRET_ACCESS_KEY\"] = minio_secret_key\n",
    "        os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = f\"https://home.{EZAF_ENV}.com:31900\"\n",
    "        os.environ['MLFLOW_S3_IGNORE_TLS'] = \"true\"\n",
    "        os.environ['MLFLOW_TRACKING_TOKEN'] = token\n",
    "        mlflow.set_tracking_uri(f\"https://mlflow.{EZAF_ENV}.com\")\n",
    "        mlflow.set_experiment(exp_name)\n",
    "        mlflow.set_tag('mlflow.user', user)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to set the experiment: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439f97e2-4c39-4464-b3e8-c778b96d28da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a new MLFlow experiment or re-use an existing one\n",
    "get_or_create_experiment('question-answering')\n",
    "\n",
    "# Log the Chroma DB files as an artifact of the experiment\n",
    "mlflow.log_artifact(f\"{os.getcwd()}/db\")\n",
    "\n",
    "# Retrieve the URI of the artifact\n",
    "uri = mlflow.get_artifact_uri(\"db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c25e4d-4803-4464-80d9-6b62ad2b4d49",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Creating and Submitting the Inference Service\n",
    "\n",
    "In the final section of this Notebook, you will create and submit an Inference Service using a YAML template and a Python subprocess. The steps in this section include:\n",
    "\n",
    "1. Creating the YAML Template: You will create a YAML file that defines the specifications of our Inference Service. This includes details like the name of the service, the Docker image to use, and other configuration settings. You will store this YAML into a file that you can explore and submit.\n",
    "1. Submitting the YAML Template: Once your YAML template is ready, you need to submit it to KServe for deployment. To do this, you will use a Python subprocess to run a shell command that submits our YAML template to KServe.\n",
    "\n",
    "By the end of this section, you will have a running Inference Service that is ready to receive user queries and provide context for answering them using the Vector Store. This marks the completion of our journey, from transforming unstructured text data into structured vector embeddings, to creating a scalable service that can provide context based on those embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05d4c9d-72e4-491a-ad99-f6f533f0ef94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "isvc = \"\"\"\n",
    "apiVersion: v1\n",
    "kind: Secret\n",
    "metadata:\n",
    "  name: minio-secret\n",
    "type: Opaque\n",
    "data:\n",
    "  EZAF_ENV: {0}\n",
    "  MINIO_ACCESS_KEY: {1}\n",
    "  MINIO_SECRET_KEY: {2}\n",
    "\n",
    "---\n",
    "apiVersion: serving.kserve.io/v1beta1\n",
    "kind: InferenceService\n",
    "metadata:\n",
    "  name: vectorstore\n",
    "spec:\n",
    "  predictor:\n",
    "    containers:\n",
    "    - name: kserve-container\n",
    "      image: {3}\n",
    "      imagePullPolicy: Always\n",
    "      args:\n",
    "      - --persist-uri\n",
    "      - {4}\n",
    "      env:\n",
    "      - name: TRANSFORMERS_CACHE\n",
    "        value: /src\n",
    "      - name: SENTENCE_TRANSFORMERS_HOME\n",
    "        value: /src\n",
    "      - name: EZAF_ENV\n",
    "        valueFrom:\n",
    "          secretKeyRef:\n",
    "            key: EZAF_ENV\n",
    "            name: minio-secret\n",
    "      - name: MINIO_ACCESS_KEY\n",
    "        valueFrom:\n",
    "          secretKeyRef:\n",
    "            key: MINIO_ACCESS_KEY\n",
    "            name: minio-secret\n",
    "      - name: MINIO_SECRET_KEY\n",
    "        valueFrom:\n",
    "          secretKeyRef:\n",
    "            key: MINIO_SECRET_KEY\n",
    "            name: minio-secret\n",
    "\"\"\".format(encode_base64(ezaf_env),\n",
    "           encode_base64(minio_access_key),\n",
    "           encode_base64(minio_secret_key),\n",
    "           predictor,\n",
    "           uri)\n",
    "\n",
    "with open(\"vectorstore/isvc.yaml\", \"w\") as f:\n",
    "    f.write(isvc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68631d06-ea76-4159-a08b-57e76850ff56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "subprocess.run([\"kubectl\", \"apply\", \"-f\", \"vectorstore/isvc.yaml\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4e2d08-8e09-4c9e-826c-0f0dfdc2d3f3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Conclusion and Next Steps\n",
    "\n",
    "Congratulations! You've successfully navigated through the process of logging the Chroma DB files as artifacts using MLFlow, creating a custom Docker image, and setting up an Inference Service with KServe that retrieves these artifacts to serve your Vector Store. This Inference Service forms the backbone of our question-answering application, enabling us to efficiently answer queries based on the document embeddings we generated previously.\n",
    "\n",
    "From here, there are two paths you can choose:\n",
    "\n",
    "- Testing the Vector Store Inference Service: If you'd like to test the Vector Store Inference Service that you've just created, you can proceed to our third (optional) Notebook. This Notebook provides a step-by-step guide on how to invoke the Inference Service and validate its performance.\n",
    "- Creating the LLM Inference Service: Alternatively, if you're ready to move on to the next stage of the project, you can jump straight to our fourth Notebook. In this Notebook, we'll guide you through the process of creating an Inference Service for the Large Language Model (LLM), which will work in conjunction with the Vector Store Inference Service to provide answers to user queries."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-question-answering]",
   "language": "python",
   "name": "conda-env-.conda-question-answering-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
