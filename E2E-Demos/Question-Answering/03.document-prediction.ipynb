{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "972db2df-307f-4492-80c6-e84082d778f2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Invoking and Testing the Vector Store Inference Service (Optional)\n",
    "\n",
    "Welcome to the third part of our tutorial series on building a question-answering application over a corpus of private documents using Large Language Models (LLMs). In our previous Notebooks, you've embarked on the journey of transforming unstructured text data into structured vector embeddings and deploying an Inference Service to serve the Vector Store that holds these embeddings.\n",
    "\n",
    "In this optional Notebook, you will focus on invoking the Vector Store Inference Service you've created and testing its performance. This is an essential step, as it allows us to verify the functionality of our service and observe how it performs in practice. Throughout this Notebook, we will guide you on how to construct suitable requests, communicate with the service, and interpret the responses.\n",
    "\n",
    "By the end of this Notebook, you will gain practical insights into the workings of the Vector Store Inference Service and will be well-prepared to integrate it into a larger system, alongside the Large Language Model Inference Service that we will create in the subsequent Notebook.\n",
    "\n",
    "Let's get started! As always, let's import the libraries you'll need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428fd850-d35a-476f-ba05-b11763ddec68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7de36b-a6cf-4896-956a-b3f9ca1faaa8",
   "metadata": {
    "tags": []
   },
   "source": [
    "To invoke the Vector Store inference service you'll need to authenticate yourself. To this end, fill the following boxes with your credentials and the EzAF cluster domain that the inference services lives to retrieve a token you can use for subsequent requests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bfa607-60e0-4cb6-993c-38d54a08fa32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add heading\n",
    "heading = widgets.HTML(\"<h2>Serving Credentials</h2>\")\n",
    "display(heading)\n",
    "\n",
    "ezaf_env_input = widgets.Text(description='EZAF Env:')\n",
    "namespace_input = widgets.Text(description='Namespace:')\n",
    "username_input = widgets.Text(description='Username:')\n",
    "password_input = widgets.Password(description='Password:')\n",
    "submit_button = widgets.Button(description='Submit')\n",
    "success_message = widgets.Output()\n",
    "\n",
    "ezaf_env = None\n",
    "namespace = None\n",
    "username = None\n",
    "password = None\n",
    "\n",
    "def submit_button_clicked(b):\n",
    "    global ezaf_env, namespace, username, password\n",
    "    ezaf_env = ezaf_env_input.value\n",
    "    namespace = namespace_input.value\n",
    "    username = username_input.value\n",
    "    password = password_input.value\n",
    "    with success_message:\n",
    "        success_message.clear_output()\n",
    "        print(\"Credentials submitted successfully!\")\n",
    "    submit_button.disabled = True\n",
    "\n",
    "submit_button.on_click(submit_button_clicked)\n",
    "\n",
    "# Set margin on the submit button\n",
    "submit_button.layout.margin = '20px 0 20px 0'\n",
    "\n",
    "# Display inputs and button\n",
    "display(ezaf_env_input, namespace_input, username_input, password_input, submit_button, success_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1e6e6d-9739-4613-b5e5-833c7976fd20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EZAF_ENV = ezaf_env \n",
    "token_url = f\"https://keycloak.{EZAF_ENV}.com/realms/UA/protocol/openid-connect/token\"\n",
    "\n",
    "data = {\n",
    "    \"username\" : username,\n",
    "    \"password\" : password,\n",
    "    \"grant_type\" : \"password\",\n",
    "    \"client_id\" : \"ua-grant\",\n",
    "}\n",
    "\n",
    "token_responce = requests.post(token_url, data=data, allow_redirects=True, verify=False)\n",
    "\n",
    "token = token_responce.json()[\"access_token\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f8bb43-af00-4dae-bf22-dec236bcafe7",
   "metadata": {
    "tags": []
   },
   "source": [
    "Finally, you need to construct the URL you'll hit and define the payload for the POST request you'll send. For this example. you'll be using the V1 inference protocol, which is described below:\n",
    "\n",
    "| API          | Verb | Path                          | Request Payload   | Response Payload                  |\n",
    "|--------------|------|-------------------------------|-------------------|-----------------------------------|\n",
    "| List Models  | GET  | /v1/models                    |                   | {\"models\": [<model_name>]}        |\n",
    "| Model Ready  | GET  | /v1/models/<model_name>       |                   | {\"name\": <model_name>,\"ready\": $bool} |\n",
    "| Predict      | POST | /v1/models/<model_name>:predict | {\"instances\": []}** | {\"predictions\": []}              |\n",
    "| Explain      | POST | /v1/models/<model_name>:explain | {\"instances\": []}** | {\"predictions\": [], \"explanations\": []} |\n",
    "\n",
    "** Payload is optional\n",
    "\n",
    "We want to invoke the `predict` API. So let's use a simple query to test our service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173e2ebd-5e3b-4289-8358-9406ba816921",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NAMESPACE = namespace\n",
    "DEPLOYMENT_NAME = \"vectorstore\"\n",
    "MODEL_NAME = DEPLOYMENT_NAME\n",
    "SVC = f'{DEPLOYMENT_NAME}-predictor-default.{NAMESPACE}.{EZAF_ENV}.com'\n",
    "URL = f\"https://{SVC}/v1/models/{MODEL_NAME}:predict\"\n",
    "\n",
    "print(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78da091c-9fce-4f91-8382-e5c785bdf24f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = {\n",
    "  \"instances\": [{\n",
    "      \"question\": \"Who's Ada Lovelace?\"\n",
    "  }]\n",
    "}\n",
    "\n",
    "headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "\n",
    "response = requests.post(URL, json=data, headers=headers, verify=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461fdac2-cacb-40cc-bf2d-d1548072bb90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6c9e0e-6d17-4d15-ba4e-e353cc1cd3c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Conclusion and Next Steps\n",
    "\n",
    "Well done! Through this Notebook, you've successfully interacted with and tested the Vector Store Inference Service. You've learned how to construct and send requests to the service and how to interpret the responses. This hands-on experience is crucial as it provides a practical understanding of the service's operation, preparing you for real-world applications.\n",
    "\n",
    "In the next Notebook, you will extend our question-answering system by creating an Inference Service for the Large Language Model (LLM). The LLM Inference Service will work in conjunction with the Vector Store Inference Service to provide comprehensive and accurate answers to user queries."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-question-answering]",
   "language": "python",
   "name": "conda-env-.conda-question-answering-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
